{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "from keras.utils import to_categorical # type: ignore\n",
    "from keras.preprocessing.sequence import pad_sequences # type: ignore\n",
    "from tensorflow.keras.models import load_model # type: ignore\n",
    "import tensorflow as tf # type: ignore\n",
    "from tensorflow.keras import Sequential # type: ignore\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, BatchNormalization # type: ignore\n",
    "from tensorflow.keras.layers import LSTM , Dense, Input # type: ignore\n",
    "from tensorflow.keras.models import Model # type: ignore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_to_grayscale(input_path):\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    video = []\n",
    "    count = 0\n",
    "    while count<frame_count:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        img = cv2.resize(image, (128, 128), interpolation=cv2.INTER_AREA)\n",
    "        video.append(img)  \n",
    "#Display resized grayscale videos\n",
    "    for i in range (0,len(video)):\n",
    "        cv2.imshow(\"image\",video[i])\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "            break\n",
    "    print(len(video))\n",
    "    return video\n",
    "\n",
    "def video_to_grayscale_extract_frames(input_path, num_frames):\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    #print(\"Original frame count \", frame_count)\n",
    "    video = []\n",
    "    count = 0\n",
    "    total_frames = min(frame_count, num_frames)\n",
    "    frame_interval = max(frame_count // total_frames, 1)\n",
    "    out_frame_count = 0\n",
    "    while count < frame_count:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        img = cv2.resize(image, (128, 128), interpolation=cv2.INTER_AREA)\n",
    "        video.append(img)\n",
    "        # Increment output frame count and break if reached desired number of frames\n",
    "        out_frame_count += 1\n",
    "        if out_frame_count >= total_frames:\n",
    "            break\n",
    "    cap.release()\n",
    "    return np.stack(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_to_3d(input_data):\n",
    "    # Reshape array to 3D\n",
    "    \"\"\"reshaped_data = np.squeeze(input_data)  # Remove single-dimensional entries\n",
    "    reshaped_data = np.reshape(reshaped_data, (reshaped_data.shape[0], -1, 1))\n",
    "    return reshaped_data\"\"\"\n",
    "    reshaped_tensor = tf.squeeze(input_data)\n",
    "    # Reshape the tensor to 3D\n",
    "    reshaped_tensor = tf.reshape(reshaped_tensor, (tf.shape(reshaped_tensor)[0], -1, 1))\n",
    "    return reshaped_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds difference of two input frames\n",
    "def difference (frame1, frame2):\n",
    "    frame_difference = frame1 - frame2\n",
    "    return frame_difference\n",
    "\n",
    "#Difference layer of FDCNN\n",
    "def forward_difference(video):\n",
    "    fdiff = []\n",
    "    for i in range (0,(len(video)-1)):\n",
    "        img1 = video[i]\n",
    "        img2 = video[i+1]\n",
    "        frame_difference = difference(img1,img2)\n",
    "        fdiff.append(frame_difference)\n",
    "    return fdiff\n",
    "\n",
    "#Difference layer of PDCNN\n",
    "def post_difference(video_feature):\n",
    "    pdiff = []\n",
    "    for i in range (0,(len(video_feature)-1)):\n",
    "        img1 = video_feature[i]\n",
    "        img2 = video_feature[i+1]\n",
    "        feature_difference = difference(img1,img2)\n",
    "        pdiff.append(feature_difference)\n",
    "    return pdiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction and fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(input_shape):\n",
    "\n",
    "    cnn = Sequential()\n",
    "\n",
    "    cnn.add(tf.keras.Input(shape=input_shape))\n",
    "    cnn.add(Conv2D(filters=2, kernel_size=(5, 5), strides=(1, 1), padding='valid', activation='elu'))\n",
    "    cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    cnn.add(Conv2D(filters=4, kernel_size=(3, 3), strides=(1, 1), padding='valid', activation='elu'))\n",
    "    cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    cnn.add(Conv2D(filters=8, kernel_size=(3, 3), strides=(1, 1), padding='valid', activation='elu'))\n",
    "    cnn.add(AveragePooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    cnn.add(Conv2D(filters=48, kernel_size=(3, 3), strides=(1, 1), padding='valid', activation='elu'))\n",
    "    cnn.add(AveragePooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    cnn.add(Conv2D(filters=24, kernel_size=(1, 1), strides=(1, 1), padding='valid', activation='elu'))\n",
    "    cnn.add(AveragePooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    cnn.add(Conv2D(filters=48, kernel_size=(3, 3), strides=(1, 1), padding='valid', activation='elu'))\n",
    "    cnn.add(BatchNormalization())\n",
    "    cnn.compile(loss=\"mse\", optimizer = \"adam\")\n",
    "    return cnn\n",
    "    #cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape=(128,128,1)\n",
    "#CNN model for FDCNN\n",
    "fdcnn = create_cnn_model(input_shape)\n",
    "#CNN model for PDCNN\n",
    "pdcnn = create_cnn_model(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracts pixel differene features\n",
    "def fdfeature_extraction(preprocessed_video,fdcnn):\n",
    "    forward_diff = forward_difference(preprocessed_video)\n",
    "    #display_forward_difference(preprocessed_video)\n",
    "    fdfeature = []\n",
    "    for frame in forward_diff:\n",
    "        frame = frame.reshape((128, 128, 1))  \n",
    "        fdf = fdcnn.predict(frame[np.newaxis, ...])\n",
    "        fdfeature.append(fdf)\n",
    "    return fdfeature\n",
    "\n",
    "#Extracts deep differene features\n",
    "def pdfeature_extraction(preprocessed_video,pdcnn):\n",
    "    pdfeature = []\n",
    "    for frame in preprocessed_video:\n",
    "        frame = frame.reshape((128, 128, 1))  \n",
    "        pdf = pdcnn.predict(frame[np.newaxis, ...])\n",
    "        pdfeature.append(pdf)\n",
    "    post_diff = post_difference(pdfeature)\n",
    "    #display_post_difference(pdfeature)\n",
    "    return post_diff\n",
    "\n",
    "#Concatenates pixel difference features and deep difference features\n",
    "def feature_fusion(preprocessed_video):\n",
    "    pixel_difference_feature = fdfeature_extraction(preprocessed_video,fdcnn)\n",
    "    deep_difference_feature = pdfeature_extraction(preprocessed_video,pdcnn)\n",
    "    msd_feature = []\n",
    "    for pdf, ddf in zip(pixel_difference_feature, deep_difference_feature):\n",
    "        concatenated_feature = tf.concat([pdf, ddf], axis=-1)\n",
    "        msd_feature.append(concatenated_feature)\n",
    "    return tf.stack(msd_feature, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm():\n",
    "    input_layer = Input(shape=(None,96))\n",
    "    lstm_layer = LSTM(144, dropout=0.2, recurrent_dropout=0.2)(input_layer)\n",
    "    #attention_output = Attention(use_scale=False, dropout=0.2)([lstm_layer, lstm_layer])\n",
    "    output_layer = Dense(1, activation='softmax')(lstm_layer)\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path,numframes):\n",
    "    x = []\n",
    "    y = []\n",
    "    for folder in [\"Original\", \"Forged\"]:\n",
    "        folder_path = os.path.join(data_path, folder)\n",
    "        for filename in os.listdir(folder_path):\n",
    "            video_path = os.path.join(folder_path, filename)\n",
    "            preprocessed_video = video_to_grayscale_extract_frames(video_path, numframes)\n",
    "            \n",
    "            # Check if the video has 100 frames, otherwise skip it\n",
    "            if len(preprocessed_video) != numframes:\n",
    "                continue\n",
    "            \n",
    "            msd = feature_fusion(preprocessed_video)\n",
    "            msd_reshaped = reshape_to_3d(msd)\n",
    "            x.append(msd_reshaped)\n",
    "            if folder == \"Original\":\n",
    "                y.append(0)\n",
    "            else:\n",
    "                y.append(1)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"D:/Final_Project/Dataset/ForgeryDataset/Insertion1/Training\"\n",
    "x_train, y_train = load_data(train_data_path,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path = \"D:/Final_Project/Dataset/ForgeryDataset/Insertion1/Testing\"\n",
    "x_test, y_test = load_data(test_data_path,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vfd_model = lstm()\n",
    "#vfd_model = load_model(\"C:/Users/Preethi/Documents/InterframeVideoForgeryDetection/models/vfd_model.keras\")\n",
    "vfd_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "vfd_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vfd_model.fit(x_train, y_train, epochs=10, batch_size=16, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = vfd_model.predict(x_test)\n",
    "print(len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vfd_model.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vidpath = \"D:/Final_Project/Dataset/ForgeryDataset/Deletion/Training/Original/original_train (40).avi\"\n",
    "preprocessed_video = video_to_grayscale(vidpath)\n",
    "msd = feature_fusion(preprocessed_video)\n",
    "msd_reshaped = reshape_to_3d(msd)\n",
    "msd_re = np.array([msd_reshaped])\n",
    "print(vfd_model.predict(msd_re))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vfd_model.save(\"C:/Users/Preethi/Documents/InterframeVideoForgeryDetection/models/vfd_model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
